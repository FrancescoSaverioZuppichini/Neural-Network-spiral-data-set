\relax 
\abx@aux@sortscheme{nty}
\abx@aux@refcontext{nty/global/}
\@writefile{toc}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lof}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{lot}{\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax }
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {1}The Perceptron}{1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Question 1}{1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1}Vectorized equations}{1}}
\newlabel{eq:vectorized_perceptron}{{1}{1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.2}Mean Squared Error}{1}}
\newlabel{eq: MSE_perceptron}{{2}{1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.3}Derivate of the error with respect to the weights}{1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.4}Compute the new weight values after one step using a learning rate of 0.02}{1}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.5}Gradient Descend}{2}}
\newlabel{eq: GD}{{4}{2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Implement the MSE and dMSE}{2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Implement the function forward and backward}{2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Implement the \emph  {run\_part1}}{2}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \emph  {run\_part1} plot\relax }}{2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:runPart1}{{1}{2}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {2}A Neural Network}{3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.1} Implement the activation functions}{3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Question 2}{3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Forward pass}{3}}
\newlabel{eq:forwardPass}{{5}{3}}
\newlabel{eq:forwardPassVectorized}{{6}{3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Delta rules}{3}}
\newlabel{eq: deltaRule_1}{{7}{3}}
\newlabel{eq:deltaRule_2}{{8}{3}}
\newlabel{eq:deltaRule}{{9}{3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}Derivatives of the weights}{3}}
\newlabel{eq:derivativesWeigthDeltas}{{11}{3}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Implement the functions forward and backward of the Neural Network class.}{4}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Train Network}{4}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.1} Split the data into a train set and a test set}{4}}
\newlabel{fig: train_test_set}{{\caption@xref {fig: train_test_set}{ on input line 144}}{4}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Train set and Test set\relax }}{4}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.2}Initialise the weights randomly and find a good learning rate to train}{4}}
\newlabel{fig: learning_rate_convergence}{{\caption@xref {fig: learning_rate_convergence}{ on input line 157}}{5}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Learning rate convergences\relax }}{5}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.3}Plot the learning rate of at least 5 different learning rates on both sets}{5}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.4}Plot the boundary of the model which converged the most on the training data}{5}}
\newlabel{fig: NN_MSE_002_boundary}{{\caption@xref {fig: NN_MSE_002_boundary}{ on input line 168}}{6}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Boundary of the model with MSE less than 0.02\relax }}{6}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Explain if this is a good model for the data}{6}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {section}{\numberline {3}Further Improvements}{6}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Generic Implementation}{6}}
\abx@aux@cite{Ruder}
\abx@aux@segm{0}{0}{Ruder}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Initial weight and bias}{7}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Performance}{7}}
\newlabel{table: performance_NN_BNN}{{\caption@xref {table: performance_NN_BNN}{ on input line 210}}{7}}
\@writefile{lot}{\defcounter {refsection}{0}\relax }\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces \relax }}{7}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Stochastic Gradient Descent}{7}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Momentum}{7}}
\newlabel{eq: momentum}{{12}{7}}
\abx@aux@cite{AdaGrad}
\abx@aux@segm{0}{0}{AdaGrad}
\newlabel{fig:momentum}{{\caption@xref {fig:momentum}{ on input line 237}}{8}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Gradient Discent vs Momentum\relax }}{8}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}Adagrad}{8}}
\newlabel{fig: adagrad}{{\caption@xref {fig: adagrad}{ on input line 255}}{9}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Gradient Discent vs Adagrad\relax }}{9}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.7}Network deep}{9}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.8}Stochastic Gradient Descent}{10}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.9}Dropout}{10}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.10}Results}{10}}
\newlabel{fig: results_boundary}{{\caption@xref {fig: results_boundary}{ on input line 309}}{11}}
\@writefile{lof}{\defcounter {refsection}{0}\relax }\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Boundary of the model\relax }}{11}}
\@writefile{toc}{\defcounter {refsection}{0}\relax }\@writefile{toc}{\contentsline {subsection}{\numberline {3.11}Save and Load}{11}}
\abx@aux@refcontextdefaultsdone
\abx@aux@defaultrefcontext{0}{AdaGrad}{nty/global/}
\abx@aux@defaultrefcontext{0}{Ruder}{nty/global/}
